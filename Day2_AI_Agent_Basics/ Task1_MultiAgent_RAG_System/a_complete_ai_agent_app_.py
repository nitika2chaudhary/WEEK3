# -*- coding: utf-8 -*-
"""a complete AI agent app .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JrU9zS5j7LXuMMjokxFDDCW2HYjYlC7C
"""

!pip install langchain langchain-community faiss-cpu sentence-transformers

from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings

import gradio as gr
from typing import List
import os



from langchain.text_splitter import CharacterTextSplitter
from langchain.docstore.document import Document

from transformers import pipeline


salary_text = """
Salary is structured into monthly and annual components.
Annual salary is monthly salary times 12
Deductions may include income tax, provident fund, and professional tax.
Net salary is credited monthly after deductions.
"""

insurance_text = """
Insurance benefits include health coverage, accident coverage, and hospitalization.
Premiums are paid monthly or annually.
Claims are submitted with bills and reports via the company/insurer portal.
Coverage typically lists inclusions, exclusions, and claim limits.
"""

docs = [
    Document(page_content=salary_text.strip(), metadata={"topic": "salary"}),
    Document(page_content=insurance_text.strip(), metadata={"topic": "insurance"}),
]


splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=30)
split_docs: List[Document] = splitter.split_documents(docs)

embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
vectorstore = FAISS.from_documents(split_docs, embeddings)


llm_pipe = pipeline("text2text-generation", model="google/flan-t5-small")

def generate(text: str, max_new_tokens: int = 180) -> str:
    out = llm_pipe(text, max_new_tokens=max_new_tokens, temperature=0.1, do_sample=False)
    return out[0]["generated_text"].strip()


def retrieve_context(topic: str, query: str, k: int = 4) -> str:
    # FAISS doesn't do metadata filters natively ‚Üí retrieve then filter
    hits = vectorstore.similarity_search(query, k=k)
    hits = [h for h in hits if h.metadata.get("topic") == topic]
    return " ".join([h.page_content for h in hits])

def salary_agent(query: str) -> str:
    ctx = retrieve_context("salary", query)
    if not ctx:
        return "I can only answer salary-related questions."
    prompt = (
        "You are an HR assistant. Answer ONLY using the provided salary context. "
        "If the question is unrelated to salary, reply that you only handle salary.\n\n"
        f"Context:\n{ctx}\n\nQuestion: {query}\nAnswer succinctly:"
    )
    return generate(prompt)

def insurance_agent(query: str) -> str:
    ctx = retrieve_context("insurance", query)
    if not ctx:
        return "I can only answer insurance-related questions."
    prompt = (
        "You are an HR benefits assistant. Answer ONLY using the provided insurance context. "
        "If the question is unrelated to insurance, reply that you only handle insurance.\n\n"
        f"Context:\n{ctx}\n\nQuestion: {query}\nAnswer succinctly:"
    )
    return generate(prompt)

# -----------------------------
# 5) Coordinator (router)
# -----------------------------
def coordinator(query: str) -> str:
    q = query.lower()
    # simple keyword routing first
    if any(w in q for w in ["salary", "pay", "deduction", "ctc", "net"]):
        return salary_agent(query)
    if any(w in q for w in ["insurance", "policy", "coverage", "premium", "claim"]):
        return insurance_agent(query)
    # fallback: choose the topic with more retrieved context
    ctx_salary = retrieve_context("salary", query)
    ctx_ins = retrieve_context("insurance", query)
    if len(ctx_ins) > len(ctx_salary):
        return insurance_agent(query) if ctx_ins else "Please ask about salary or insurance."
    else:
        return salary_agent(query) if ctx_salary else "Please ask about salary or insurance."

# -----------------------------
# 6) Two sample queries (Deliverable)
# -----------------------------
def run_samples() -> str:
    q1 = "How do I calculate annual salary?"
    a1 = coordinator(q1)
    q2 = "What is included in my insurance policy?"
    a2 = coordinator(q2)
    return (
        "Sample 1\n"
        f"Q: {q1}\nA: {a1}\n\n"
        "Sample 2\n"
        f"Q: {q2}\nA: {a2}\n"
    )

# -----------------------------
# 7) Gradio UI (with simple memory)
# -----------------------------
with gr.Blocks(title="Multi-Agent RAG: Salary + Insurance") as demo:
    gr.Markdown("## üßë‚Äçüíº Multi-Agent RAG (Salary + Insurance)\nAsk about salary or insurance. The coordinator routes to the right agent.")
    with gr.Row():
        chat = gr.Chatbot(label="Chat (memory in this session)")
    with gr.Row():
        inp = gr.Textbox(placeholder="e.g., What is included in my insurance policy?", scale=4)
        btn = gr.Button("Ask", variant="primary", scale=1)
    samples = gr.Button("Run Sample Queries")
    clear = gr.Button("Clear Chat")

    state = gr.State([])  # list of (user, assistant)

    def ask_fn(user_msg, hist):
        ans = coordinator(user_msg)
        hist = hist + [(user_msg, ans)]
        return hist, hist

    def samples_fn(hist):
        text = run_samples()
        # display samples as a single assistant message
        hist = hist + [("", text)]
        return hist, hist

    btn.click(ask_fn, inputs=[inp, chat], outputs=[chat, state])
    samples.click(samples_fn, inputs=[chat], outputs=[chat, state])
    clear.click(lambda: ([], []), None, [chat, state])

demo.launch(share=True)